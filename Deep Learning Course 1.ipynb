{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course : Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\n",
    "## Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 16:00:46.728629: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-21 16:00:46.728675: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The 'Hello World' of Neural Networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 04:03:02.972525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-20 04:03:02.972565: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-20 04:03:02.972589: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-839790): /proc/driver/nvidia/version does not exist\n",
      "2022-06-20 04:03:02.973019: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.2963\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.1388\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2242\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5010\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9284\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4743\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1135\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8262\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5969\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4131\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2654\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1459\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0489\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9695\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9041\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8497\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8041\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7655\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7323\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7035\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6783\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6559\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6357\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6174\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6006\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5850\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5704\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5567\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5437\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5313\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5194\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5079\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4969\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4862\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4759\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4658\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4560\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4464\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4371\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4280\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4192\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4105\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4020\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3937\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3856\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3776\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3698\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3622\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3548\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3475\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3403\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3333\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3265\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3198\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3132\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3068\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3005\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2943\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2882\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2823\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2765\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2708\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2653\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2598\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2545\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2493\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2441\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2391\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2342\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2294\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2247\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2201\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2156\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2111\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2068\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2025\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1984\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1943\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1903\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1864\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1826\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1788\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1752\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1716\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1680\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1646\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1612\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1579\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1546\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1515\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1484\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1453\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1423\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1394\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1365\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1337\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1310\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1283\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1257\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1231\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1206\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1181\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1157\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1133\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1109\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1087\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1021\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1000\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0980\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0959\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0940\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0920\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0902\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0883\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0865\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0847\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0830\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0813\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0796\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0780\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0764\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0748\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0733\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0718\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0703\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0688\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0674\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0660\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0647\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0634\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0621\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0608\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0595\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0583\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0571\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0559\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0548\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0537\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0526\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0515\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0504\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0494\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0484\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0474\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0464\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0455\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0445\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0436\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0427\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0418\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0410\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0401\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0393\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0385\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0377\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0369\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0362\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0354\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0347\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0340\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0333\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0326\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0319\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0313\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0306\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0300\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0294\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0288\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0282\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0276\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0271\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0265\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0260\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0254\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0249\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0244\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0239\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0234\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0229\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0224\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0220\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0215\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0211\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0207\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0198\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0194\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0190\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0186\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0182\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0179\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0175\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0171\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0168\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0164\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0161\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0158\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0154\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0151\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0148\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0145\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0142\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0139\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0131\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0128\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0126\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0123\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0120\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0118\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0116\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0113\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0109\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0106\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0104\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0102\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0100\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0098\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0096\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0094\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0090\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0088\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0086\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0085\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0070\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0065\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0057\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0050\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0043\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0040\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0038\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0036\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0035\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0033\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0033\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0031\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0029\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0029\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0024\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0023\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0022\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0019\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0017\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0016\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0015\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0010\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.9780e-04\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7731e-04\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.5723e-04\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.3757e-04\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1831e-04\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9945e-04\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.8097e-04\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6287e-04\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4515e-04\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.2779e-04\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1079e-04\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.9413e-04\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7782e-04\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6184e-04\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4619e-04\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3087e-04\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1585e-04\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0115e-04\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8675e-04\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7264e-04\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5882e-04\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4529e-04\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.3204e-04\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1906e-04\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0634e-04\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9388e-04\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8169e-04\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.6974e-04\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5804e-04\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4657e-04\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.3534e-04\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2435e-04\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.1358e-04\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0303e-04\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9270e-04\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8258e-04\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.7266e-04\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6296e-04\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.5345e-04\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4413e-04\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3501e-04\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2607e-04\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1732e-04\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0875e-04\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0035e-04\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9213e-04\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8408e-04\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7618e-04\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6846e-04\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.6089e-04\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.5347e-04\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.4622e-04\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3910e-04\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3214e-04\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2532e-04\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.1863e-04\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1209e-04\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0568e-04\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9940e-04\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9325e-04\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8723e-04\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8133e-04\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7555e-04\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6989e-04\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6435e-04\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5892e-04\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5360e-04\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4839e-04\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4329e-04\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3829e-04\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3339e-04\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2860e-04\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2390e-04\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1931e-04\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1480e-04\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1039e-04\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0607e-04\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0183e-04\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9769e-04\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9363e-04\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8965e-04\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8575e-04\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8194e-04\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7820e-04\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7454e-04\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7096e-04\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6744e-04\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6400e-04\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6064e-04\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5734e-04\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5410e-04\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5094e-04\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4784e-04\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4480e-04\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4183e-04\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3891e-04\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3606e-04\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3327e-04\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3053e-04\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2785e-04\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2522e-04\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2265e-04\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2013e-04\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1766e-04\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1524e-04\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1288e-04\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1056e-04\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0829e-04\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0606e-04\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0389e-04\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0175e-04\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.9663e-05\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7616e-05\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.5610e-05\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.3647e-05\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1723e-05\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9840e-05\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7995e-05\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6187e-05\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4417e-05\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.2683e-05\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.0984e-05\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.9320e-05\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7692e-05\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6095e-05\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4533e-05\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.3003e-05\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1503e-05\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0034e-05\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8595e-05\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.7187e-05\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5807e-05\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.4455e-05\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3131e-05\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1834e-05\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.0564e-05\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.9321e-05\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8102e-05\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6908e-05\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5740e-05\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.4594e-05\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3473e-05\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2374e-05\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.1299e-05\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0245e-05\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9213e-05\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8203e-05\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7213e-05\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6243e-05\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.5293e-05\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4362e-05\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3452e-05\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2560e-05\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1686e-05\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0829e-05\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9990e-05\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9170e-05\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8365e-05\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7577e-05\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6805e-05\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6049e-05\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.5309e-05\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4583e-05\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3872e-05\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3177e-05\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.2496e-05\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1828e-05\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1175e-05\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0534e-05\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "[[18.983877]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "xData = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "yData = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "model.fit(xData, yData, epochs=500)\n",
    "\n",
    "print(model.predict([10.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Introduction to Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be using the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['T-shirt/Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   3  68  92  80  76  59  39   7   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   1   3   0 112 154 138 148 173 182 147 121 115 104  20   0   0   2   2   0   0   0   0   1   0   0]\n",
      " [  1   0   3   0   0  55 255   0   0   0   7  60 129 167 155 177 165 100  13   0   0   1   1   2   1   3   2   0]\n",
      " [  0   0   0   0   0 160  85   0   0   0   0   0   0   0  40 107 167 150 180  43   0   0   0   0   0   0   0   0]\n",
      " [  8  37  40  42  50 219  67  80  90  98  85  53  29   6   4   0 115 167 168 165  46   0  29  21  16   8   0   0]\n",
      " [ 38 130 112  94  84 102  84  77  86  86 100 112 125 140 146 130  92 172 186 190 179 100 118 114 116 109 121  35]\n",
      " [ 14  74  87  91 101 101 104 101 103 105 104 105 104 101  98 103 101  87  88  84  93 105 104 106 110 112 141  52]\n",
      " [  0  12  29  37  46  54  58  65  70  67  66  65  65  64  65  64  62  60  55  56  54  55  57  56  53  55  50   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f27c29f2fb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=320)\n",
    "\n",
    "print(train_images[23324])\n",
    "plt.imshow(train_images[23324], cmap='turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Trouser\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPyUlEQVR4nO3dXYid9bXH8d8yr5NJjUnUSYgvNUENiVh7GMJB5ZCTqqg32hupYPGAmF4otFBQSS8qeCMH2+LFoZAetemxx1JIRS/0nKoURIVqjGNeTDQ5ElNjYsybM3mdzGSdi3mUGZ29/uN+mb3N+n5gmD3P2v/ZKw/zy7P3/u/n+Zu7C8DZ75x2NwBgchB2IAnCDiRB2IEkCDuQxNTJfDAz463/OsyZMyesd3d316wNDQ2FY6dMmRLWT548GdZnzpwZ1qPZnjNnzoRjp06N/zwPHjwY1k+dOhXWz1bubuNtbyjsZnazpMclTZH0n+7+aCO/D+NbuXJlWO/t7a1ZKwVi9uzZYX3nzp1h/Yorrgjrg4ODNWvHjh0Lx1544YVh/emnnw7r77//fljPpu6n8WY2RdJ/SLpF0jJJd5rZsmY1BqC5GnnNvkLSTnf/0N0HJf1J0m3NaQtAszUS9kWS/jHq54+rbWOY2Woz22BmGxp4LAANavkbdO6+VtJaiTfogHZq5Mi+R9LFo36+qNoGoAM1Eva3JF1uZpeZ2XRJP5L0fHPaAtBsdT+Nd/chM7tf0v9qZOrtSXff2rTO8KUHH3wwrC9durRm7fTp0+HY+fPnh/W+vr6wfumll4b1GTNm1Kzt2rUrHNvV1RXWo2k9SXrkkUfCejYNvWZ39xckvdCkXgC0EB+XBZIg7EAShB1IgrADSRB2IAnCDiQxqeezoz6lufBDhw7VrJ04cSIc29/fH9ZLc90ffPBBWJ8+fXrN2tGjR8OxpdNvo383vo4jO5AEYQeSIOxAEoQdSIKwA0kQdiAJpt46QOlyzIsXLw7r0fRX6Qqupemtkugy1pI0PDxcs3bOOfGxprRfli3j+qbfBEd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCefYOcNVVV4X1aNljKV6a+Nxzzw3Hlua6S0s2ly7nPG3atJq10pLNJaV/G8biyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDP3gEWLVoU1oeGhsL6lClTatZK8+ilefLS+NL57NE8e+nfFX1+QJIuuuiisI6xGgq7me2SNCBpWNKQu/c2oykAzdeMI/u/uvuBJvweAC3Ea3YgiUbD7pL+amZvm9nq8e5gZqvNbIOZbWjwsQA0oNGn8de7+x4zu1DSS2a23d1fHX0Hd18raa0kmVl8RgeAlmnoyO7ue6rv+yU9K2lFM5oC0Hx1h93Mus3sO1/clnSTpC3NagxAczXyNL5H0rNm9sXv+W93/5+mdJXMZZddFtZPnz4d1qOlj2fMmBGOjZZUluI5fKk8Dx8tq3zkyJFwbDRHL0mzZs0K6xir7rC7+4eSvtfEXgC0EFNvQBKEHUiCsANJEHYgCcIOJMEprh1gyZIlYb207HI0vVaavvr888/D+vLly8N6NLUmxaexDgwMhGP7+/vDeukS2xiLIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME8ewdYsGBBWN+9e3dYj06BLZ2i2tXVFda3b98e1kunmc6bN69mrTSPXjq1t9Eln7PhyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDP3gGiuWipvKxydTnvcZXmqkvnq69atSqsr1mzJqzfdNNNNWulefLSks6l8+ExFkd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCefYOUDon/OTJk2E9mq8+77zzwrGlJZdfe+21sP7JJ5+E9dJceaS7uzus79ixo+7fnVHxyG5mT5rZfjPbMmrbPDN7ycx2VN/ntrZNAI2ayNP430u6+SvbHpL0irtfLumV6mcAHawYdnd/VdJX1/i5TdK66vY6Sbc3ty0AzVbva/Yed99b3d4nqafWHc1staTVdT4OgCZp+A06d3czq7nCnruvlbRWkqL7AWiteqfePjWzhZJUfd/fvJYAtEK9YX9e0t3V7bslPdecdgC0SvFpvJk9I2mlpPPN7GNJv5T0qKQ/m9k9kj6SdEcrmzzbReurS9Lx48fD+qlTp2rWLrnkknDs+vXrw3pJaZ49Otf+xIkT4diZM2eG9dJ+wVjFsLv7nTVKP2hyLwBaiI/LAkkQdiAJwg4kQdiBJAg7kASnuHaAY8eOhfXp06eH9eHh4Zq10vTV448/HtZL3n333bAe9TZt2rRwbOnU33379oV1jMWRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJ69A1xwwQVh/bPPPqv7d0fz3JK0cePGun+3JL3xxhthPTr91j2+cFHp8wVbt24N6xiLIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME8ewfYtWtXWC/NRzeyLPLUqY39CfT394f1qPdS36XPF5SWm8ZY7C0gCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJ59g5Quvb6jTfeGNb37t1bs1aaRy9dV/7o0aNhvXTt92iefXBwMBzb1dUV1qPloPF1xSO7mT1pZvvNbMuobQ+b2R4z66u+bm1tmwAaNZGn8b+XdPM423/j7tdUXy80ty0AzVYMu7u/KunQJPQCoIUaeYPufjPbVD3Nn1vrTma22sw2mNmGBh4LQIPqDftvJS2RdI2kvZJ+VeuO7r7W3XvdvbfOxwLQBHWF3d0/dfdhdz8j6XeSVjS3LQDNVlfYzWzhqB9/KGlLrfsC6AzFeXYze0bSSknnm9nHkn4paaWZXSPJJe2S9JPWtXj26+vrC+t33XVXWI/Wd4/m4CVp8eLFYf3AgQNh/ciRI2E9mkvv7u4Oxy5atCisl86lx1jFsLv7neNsfqIFvQBoIT4uCyRB2IEkCDuQBGEHkiDsQBKc4toB3nnnnbA+a9assH7ixImatTNnzoRj77333rD+5ptvhvWSBQsW1KwdP348HDswMBDWly9fHtZff/31sJ4NR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJ59g6wffv2sF6aZ1+4cGHNWulyy1dffXVYb6XSKa6l3ufOrXk1NIyDIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME8+7fAyy+/HNavu+66mrXSpZ6XLVtWT0sTFi3ZXFruubScdGmeHmNxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJJhn/xZ46qmnwvq1115bs3bq1KlwbGkuu6urK6xH16yX4mu/T50a//mdPHkyrG/atCmsY6zikd3MLjazv5nZe2a21cx+Wm2fZ2YvmdmO6jtXEgA62ESexg9J+rm7L5P0z5LuM7Nlkh6S9Iq7Xy7plepnAB2qGHZ33+vuG6vbA5K2SVok6TZJ66q7rZN0e4t6BNAE3+g1u5l9V9L3Jf1dUo+7761K+yT11BizWtLqBnoE0AQTfjfezGZLWi/pZ+7eP7rmI2c7jHvGg7uvdfded+9tqFMADZlQ2M1smkaC/kd3/0u1+VMzW1jVF0ra35oWATRD8Wm8jVzP9wlJ29z916NKz0u6W9Kj1ffnWtIh9OKLL4b1aOnj0uWYDx8+HNavvPLKsN7X1xfWT58+XbM2Y8aMcGxpuenh4eGwjrEm8pr9Okk/lrTZzPqqbWs0EvI/m9k9kj6SdEdLOgTQFMWwu/trkmodHn7Q3HYAtAoflwWSIOxAEoQdSIKwA0kQdiAJTnH9Fjh27Fjd9dJppKVTXJcsWRLWS/Ps0aWkSxo9BRZjcWQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYZz8LRJdUvuGGG8KxQ0NDYX3FihVhff369WE9Op999uzZ4djSHP2OHTvCOsbiyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDP3gFK108vLbv82GOP1aytWrUqHFs6JzxaDrpRpevCN7rcNMbiyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSUxkffaLJf1BUo8kl7TW3R83s4cl3Svps+qua9z9hVY1ejYrnVNesnHjxpq1AwcOhGPnzJkT1nt6eurq6QsHDx6sWZs1a1Y4trT+eqP7LZuJfKhmSNLP3X2jmX1H0ttm9lJV+4271/5EB4COMZH12fdK2lvdHjCzbZIWtboxAM31jV6zm9l3JX1f0t+rTfeb2SYze9LM5tYYs9rMNpjZhsZaBdCICYfdzGZLWi/pZ+7eL+m3kpZIukYjR/5fjTfO3de6e6+79zbeLoB6TSjsZjZNI0H/o7v/RZLc/VN3H3b3M5J+Jym+MiGAtiqG3cxM0hOStrn7r0dtXzjqbj+UtKX57QFolom8G3+dpB9L2mxmfdW2NZLuNLNrNDIdt0vST1rQXwqlKaZzzon/Tx4cHKxZ27ZtWzj2lltuCevz588P6yU7d+6sWVu6dGk49vDhw2F9z549dfWU1UTejX9Nko1TYk4d+BbhE3RAEoQdSIKwA0kQdiAJwg4kQdiBJLiU9LdAaZ49uiTzAw88EI49cuRIWI9On52I++67r2Zt9+7d4djNmzeH9YGBgbp6yoojO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kYe4+eQ9m9pmkj0ZtOl9SfK3j9unU3jq1L4ne6tXM3i519wvGK0xq2L/24GYbOvXadJ3aW6f2JdFbvSarN57GA0kQdiCJdod9bZsfP9KpvXVqXxK91WtSemvra3YAk6fdR3YAk4SwA0m0JexmdrOZvW9mO83soXb0UIuZ7TKzzWbW1+716ao19Pab2ZZR2+aZ2UtmtqP6Pu4ae23q7WEz21Ptuz4zu7VNvV1sZn8zs/fMbKuZ/bTa3tZ9F/Q1Kftt0l+zm9kUSR9IulHSx5LeknSnu783qY3UYGa7JPW6e9s/gGFm/yLpqKQ/uPtV1bZ/l3TI3R+t/qOc6+4PdkhvD0s62u5lvKvVihaOXmZc0u2S/k1t3HdBX3doEvZbO47sKyTtdPcP3X1Q0p8k3daGPjqeu78q6dBXNt8maV11e51G/lgmXY3eOoK773X3jdXtAUlfLDPe1n0X9DUp2hH2RZL+Mernj9VZ6727pL+a2dtmtrrdzYyjx933Vrf3SeppZzPjKC7jPZm+ssx4x+y7epY/bxRv0H3d9e7+T5JukXRf9XS1I/nIa7BOmjud0DLek2WcZca/1M59V+/y541qR9j3SLp41M8XVds6grvvqb7vl/SsOm8p6k+/WEG3+r6/zf18qZOW8R5vmXF1wL5r5/Ln7Qj7W5IuN7PLzGy6pB9Jer4NfXyNmXVXb5zIzLol3aTOW4r6eUl3V7fvlvRcG3sZo1OW8a61zLjavO/avvy5u0/6l6RbNfKO/P9J+kU7eqjR12JJ71ZfW9vdm6RnNPK07rRG3tu4R9J8Sa9I2iHpZUnzOqi3/5K0WdImjQRrYZt6u14jT9E3Seqrvm5t974L+pqU/cbHZYEkeIMOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5L4fy1eBJwt54/pAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[49840], cmap='gray')\n",
    "print(train_labels[49840], labels[train_labels[49840]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[49440], cmap='gray')\n",
    "print(train_labels[49440], labels[train_labels[49440]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that all of the values in the number are between 0 and 255. If you are training a neural network especially in image processing, for various reasons it will usually learn better if you scale all values to between 0 and 1. It's a process called _normalization_ and fortunately in Python, it's easy to normalize an array without looping. You do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:24:54.320649: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-20 14:24:54.320690: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-20 14:24:54.320711: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-839790): /proc/driver/nvidia/version does not exist\n",
      "2022-06-20 14:24:54.321157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sequential](https://keras.io/api/models/sequential/): That defines a sequence of layers in the neural network.\n",
    "\n",
    "[Flatten](https://keras.io/api/layers/reshaping_layers/flatten/): Remember earlier where our images were a 28x28 pixel matrix when you printed them out? Flatten just takes that square and turns it into a 1-dimensional array of length 784.\n",
    "\n",
    "[Dense](https://keras.io/api/layers/core_layers/dense/): Adds a layer of neurons\n",
    "\n",
    "Each layer of neurons need an [activation function](https://keras.io/api/layers/activations/) to tell them what to do. There are a lot of options, but just use these for now: \n",
    "\n",
    "[ReLU](https://keras.io/api/layers/activations/#relu-function) effectively means:\n",
    "\n",
    "```Python\n",
    "if x > 0: \n",
    "  return x\n",
    "\n",
    "else: \n",
    "  return 0\n",
    "```\n",
    "\n",
    "In other words, it only passes values 0 or greater to the next layer in the network.\n",
    "\n",
    "[Softmax](https://keras.io/api/layers/activations/#softmax-function) takes a list of values and scales these so the sum of all elements will be equal to 1. When applied to model outputs, you can think of the scaled values as the probability for that class. For example, in your classification model which has 10 units in the output dense layer, having the highest value at `index = 4` means that the model is most confident that the input clothing image is a coat. If it is at index = 5, then it is a sandal, and so forth. See the short code block below which demonstrates these concepts. You can also watch this [lecture](https://www.youtube.com/watch?v=LLux1SW--oM&ab_channel=DeepLearningAI) if you want to know more about the Softmax function and how the values are computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to softmax function: [[1. 3. 4. 2.]]\n",
      "output of softmax function: [[0.0320586  0.23688282 0.64391426 0.08714432]]\n",
      "sum of outputs: 1.0\n",
      "class with highest probability: 2\n"
     ]
    }
   ],
   "source": [
    "# Declare sample inputs and convert to a tensor\n",
    "inputs = np.array([[1.0, 3.0, 4.0, 2.0]])\n",
    "inputs = tf.convert_to_tensor(inputs)\n",
    "print(f'input to softmax function: {inputs.numpy()}')\n",
    "\n",
    "# Feed the inputs to a softmax activation function\n",
    "outputs = tf.keras.activations.softmax(inputs)\n",
    "print(f'output of softmax function: {outputs.numpy()}')\n",
    "\n",
    "# Get the sum of all values after the softmax\n",
    "sum = tf.reduce_sum(outputs)\n",
    "print(f'sum of outputs: {sum}')\n",
    "\n",
    "# Get the index with highest value\n",
    "prediction = np.argmax(outputs)\n",
    "print(f'class with highest probability: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:25:24.427317: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4943 - accuracy: 0.8271\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3735 - accuracy: 0.8644\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3345 - accuracy: 0.8780\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3139 - accuracy: 0.8854\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2963 - accuracy: 0.8903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27c20a7910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/313 [..............................] - ETA: 41s - loss: 0.2894 - accuracy: 0.9062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:25:54.042654: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3497 - accuracy: 0.8762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3496550917625427, 0.8762000203132629]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98/313 [========>.....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:26:17.999059: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.99173207e-05, 3.76577248e-07, 1.16567404e-04, 1.44963838e-06, 4.70642362e-06, 4.35593277e-02, 4.90691491e-05, 3.38718928e-02, 5.52820624e-04, 9.21813786e-01], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "\n",
    "classifications[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9218138"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label index : 9, label : Ankle boot\n"
     ]
    }
   ],
   "source": [
    "print(f\"label index : {test_labels[0]}, label : {labels[test_labels[0]]}\")\n",
    "\n",
    "# 9th index has highest probability, therefore result is 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:31:00.002402: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4739 - accuracy: 0.8309\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3580 - accuracy: 0.8692\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3224 - accuracy: 0.8818\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3001 - accuracy: 0.8893\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2811 - accuracy: 0.8953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27bd38f310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3415 - accuracy: 0.8788\n",
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)\n",
    "classifications = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label : Sneaker, Actual Label : Sneaker\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted label : {labels[np.argmax(classifications[9])]}, Actual Label : {labels[test_labels[9]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4702 - accuracy: 0.8314\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3576 - accuracy: 0.8696\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3225 - accuracy: 0.8812\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2965 - accuracy: 0.8894\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2772 - accuracy: 0.8971\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3496 - accuracy: 0.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34962698817253113, 0.8694000244140625]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4695 - accuracy: 0.8317\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3599 - accuracy: 0.8681\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3218 - accuracy: 0.8814\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2991 - accuracy: 0.8900\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2798 - accuracy: 0.8955\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2636 - accuracy: 0.9019\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2524 - accuracy: 0.9065\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2384 - accuracy: 0.9108\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2278 - accuracy: 0.9147\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2198 - accuracy: 0.9171\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3263 - accuracy: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3262684941291809, 0.8865000009536743]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4728 - accuracy: 0.8314\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3603 - accuracy: 0.8678\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3202 - accuracy: 0.8817\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2952 - accuracy: 0.8900\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2777 - accuracy: 0.8968\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2628 - accuracy: 0.9018\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2532 - accuracy: 0.9060\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2401 - accuracy: 0.9085\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2293 - accuracy: 0.9132\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2206 - accuracy: 0.9161\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2137 - accuracy: 0.9204\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2036 - accuracy: 0.9234\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1944 - accuracy: 0.9268\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1896 - accuracy: 0.9279\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1828 - accuracy: 0.9306\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1773 - accuracy: 0.9331\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1711 - accuracy: 0.9350\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1651 - accuracy: 0.9374\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1606 - accuracy: 0.9398\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1568 - accuracy: 0.9418\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1512 - accuracy: 0.9428\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1487 - accuracy: 0.9432\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1423 - accuracy: 0.9456\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1393 - accuracy: 0.9470\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1349 - accuracy: 0.9492\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1319 - accuracy: 0.9511\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1264 - accuracy: 0.9516\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1261 - accuracy: 0.9523\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1218 - accuracy: 0.9544\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1185 - accuracy: 0.9552\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4244 - accuracy: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42437782883644104, 0.8956000208854675]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=30)\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback function for high epochs\n",
    "\n",
    "class callBackMNIST(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        if (logs.get('loss') <= 0.4):\n",
    "            print(\"\\nLoss is lower than 0.4 so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "callback = callBackMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4084 - accuracy: 0.8633\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4082 - accuracy: 0.8620\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4057 - accuracy: 0.8623\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4011 - accuracy: 0.8635\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4084 - accuracy: 0.8640\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4205 - accuracy: 0.8609\n",
      "Epoch 7/50\n",
      "1859/1875 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8645\n",
      "Loss is lower than 0.4 so cancelling training!\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3996 - accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7a0d261d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=50, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5937 - accuracy: 0.8412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.593691349029541, 0.8411999940872192]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
